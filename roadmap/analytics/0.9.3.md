# 0.9.3 - Testing & Documentation

## Goal
Comprehensive testing of analytics and error tracking implementation with full documentation, migration guides, and preparation for production deployment in version 0.10.0.

## Deliverables
- [ ] E2E tests for analytics tracking (deferred to backlog)
- [x] Unit tests for analytics providers
- [ ] Integration tests for error tracking (deferred to backlog)
- [ ] Performance impact assessment (deferred to backlog)
- [x] Complete setup documentation
- [x] Migration guide for 0.10.0
- [x] Privacy policy updates
- [x] Monitoring playbook

## Implementation

### E2E Analytics Tests

```typescript
// packages/web/e2e/analytics.spec.ts
import { test, expect } from '@playwright/test';

test.describe('Analytics Tracking', () => {
  test.beforeEach(async ({ page }) => {
    // Mock Umami script
    await page.addInitScript(() => {
      window.umami = {
        track: (event: string, data: any) => {
          window.__analyticsEvents = window.__analyticsEvents || [];
          window.__analyticsEvents.push({ event, data, timestamp: Date.now() });
        }
      };
    });
  });

  test('tracks page views on navigation', async ({ page }) => {
    await page.goto('/');

    // Navigate to blog
    await page.click('a[href="/blog"]');
    await page.waitForLoadState('networkidle');

    // Check if pageview was tracked
    const events = await page.evaluate(() => window.__analyticsEvents);
    const pageviewEvents = events.filter(e => e.event === 'pageview');

    expect(pageviewEvents).toHaveLength(2);
    expect(pageviewEvents[1].data.url).toBe('/blog');
  });

  test('tracks custom events on interaction', async ({ page }) => {
    await page.goto('/');

    // Click a tracked button
    await page.click('[data-track="cta-button"]');

    const events = await page.evaluate(() => window.__analyticsEvents);
    const clickEvents = events.filter(e => e.event === 'click');

    expect(clickEvents).toHaveLength(1);
    expect(clickEvents[0].data.label).toBe('cta-button');
  });

  test('respects user consent preferences', async ({ page }) => {
    await page.goto('/privacy');

    // Disable analytics consent
    await page.click('[data-testid="analytics-consent-toggle"]');

    // Navigate to trigger tracking attempt
    await page.click('a[href="/blog"]');

    // Check that no events were tracked
    const events = await page.evaluate(() => window.__analyticsEvents);
    expect(events).toHaveLength(0);
  });

  test('tracks form submissions', async ({ page }) => {
    await page.goto('/contact');

    // Fill and submit form
    await page.fill('input[name="name"]', 'Test User');
    await page.fill('input[name="email"]', 'test@example.com');
    await page.fill('textarea[name="message"]', 'Test message');
    await page.click('button[type="submit"]');

    const events = await page.evaluate(() => window.__analyticsEvents);
    const formEvents = events.filter(e => e.event === 'form_submit');

    expect(formEvents).toHaveLength(1);
    expect(formEvents[0].data.form).toBe('contact');
  });

  test('tracks performance metrics', async ({ page }) => {
    await page.goto('/');

    // Wait for Web Vitals to be collected
    await page.waitForTimeout(3000);

    const events = await page.evaluate(() => window.__analyticsEvents);
    const performanceEvents = events.filter(e => e.event.includes('performance'));

    expect(performanceEvents.length).toBeGreaterThan(0);

    // Check for specific metrics
    const hasLCP = performanceEvents.some(e => e.data.label === 'LCP');
    const hasFCP = performanceEvents.some(e => e.data.label === 'FCP');

    expect(hasLCP).toBe(true);
    expect(hasFCP).toBe(true);
  });
});

test.describe('Error Tracking', () => {
  test.beforeEach(async ({ page }) => {
    // Mock Sentry
    await page.addInitScript(() => {
      window.Sentry = {
        captureException: (error: Error) => {
          window.__capturedErrors = window.__capturedErrors || [];
          window.__capturedErrors.push({
            message: error.message,
            stack: error.stack,
            timestamp: Date.now()
          });
          return 'mock-event-id';
        },
        captureMessage: (message: string) => {
          window.__capturedMessages = window.__capturedMessages || [];
          window.__capturedMessages.push({ message, timestamp: Date.now() });
          return 'mock-event-id';
        }
      };
    });
  });

  test('captures unhandled errors', async ({ page }) => {
    await page.goto('/');

    // Trigger an error
    await page.evaluate(() => {
      throw new Error('Test unhandled error');
    });

    const errors = await page.evaluate(() => window.__capturedErrors);
    expect(errors).toHaveLength(1);
    expect(errors[0].message).toBe('Test unhandled error');
  });

  test('captures API errors', async ({ page }) => {
    // Mock failing API
    await page.route('/api/test', route => {
      route.fulfill({
        status: 500,
        body: JSON.stringify({ error: 'Internal Server Error' })
      });
    });

    await page.goto('/');

    // Trigger API call
    await page.evaluate(() => {
      fetch('/api/test').catch(() => {});
    });

    await page.waitForTimeout(1000);

    const errors = await page.evaluate(() => window.__capturedErrors);
    expect(errors.length).toBeGreaterThan(0);
  });

  test('error boundary prevents app crash', async ({ page }) => {
    await page.goto('/test/error-boundary');

    // Click button that triggers error
    await page.click('[data-testid="trigger-error"]');

    // App should show error UI, not crash
    await expect(page.locator('text=Something went wrong')).toBeVisible();

    // Should be able to recover
    await page.click('text=Try again');
    await expect(page.locator('text=Something went wrong')).not.toBeVisible();
  });
});
```

### Unit Tests for Analytics

```typescript
// packages/web/src/lib/analytics/__tests__/events.test.ts
import { AnalyticsEvents, EventCategory, EventAction } from '../events';

describe('AnalyticsEvents', () => {
  let analytics: AnalyticsEvents;
  let mockUmami: jest.Mock;

  beforeEach(() => {
    analytics = new AnalyticsEvents();
    mockUmami = jest.fn();
    window.umami = { track: mockUmami };

    // Mock localStorage
    Storage.prototype.getItem = jest.fn((key) => {
      if (key === 'analytics_consent') return 'granted';
      return null;
    });
  });

  afterEach(() => {
    jest.clearAllMocks();
  });

  it('tracks events when consent is granted', () => {
    analytics.track({
      category: EventCategory.Navigation,
      action: EventAction.Click,
      label: 'header-link',
      value: 1
    });

    expect(mockUmami).toHaveBeenCalledWith(
      'navigation_click',
      expect.objectContaining({
        label: 'header-link',
        value: 1
      })
    );
  });

  it('blocks tracking when consent is denied', () => {
    Storage.prototype.getItem = jest.fn(() => 'denied');

    analytics.track({
      category: EventCategory.Navigation,
      action: EventAction.Click
    });

    expect(mockUmami).not.toHaveBeenCalled();
  });

  it('queues events when offline', () => {
    // Simulate offline
    Object.defineProperty(navigator, 'onLine', {
      writable: true,
      value: false
    });

    analytics = new AnalyticsEvents();

    analytics.track({
      category: EventCategory.Content,
      action: EventAction.View
    });

    // Should not send immediately
    expect(mockUmami).not.toHaveBeenCalled();

    // Simulate coming back online
    Object.defineProperty(navigator, 'onLine', { value: true });
    window.dispatchEvent(new Event('online'));

    // Should flush queue
    setTimeout(() => {
      expect(mockUmami).toHaveBeenCalled();
    }, 100);
  });

  it('generates unique session IDs', () => {
    const sessionId1 = analytics['getSessionId']();

    // Clear session storage
    sessionStorage.clear();

    const sessionId2 = analytics['getSessionId']();

    expect(sessionId1).not.toBe(sessionId2);
    expect(sessionId1).toMatch(/^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/);
  });
});

// packages/web/src/lib/analytics/__tests__/performance.test.ts
import { initPerformanceTracking } from '../performance';
import { onLCP, onFCP, onCLS } from 'web-vitals';

jest.mock('web-vitals');

describe('Performance Tracking', () => {
  it('initializes Web Vitals observers', () => {
    initPerformanceTracking();

    expect(onLCP).toHaveBeenCalled();
    expect(onFCP).toHaveBeenCalled();
    expect(onCLS).toHaveBeenCalled();
  });

  it('sends metrics with correct format', () => {
    const mockTrack = jest.fn();
    window.umami = { track: mockTrack };

    initPerformanceTracking();

    // Simulate LCP metric
    const lpcCallback = (onLCP as jest.Mock).mock.calls[0][0];
    lpcCallback({
      name: 'LCP',
      value: 2500,
      rating: 'needs-improvement',
      delta: 100,
      id: 'test-id'
    });

    expect(mockTrack).toHaveBeenCalledWith('performance_view', {
      label: 'LCP',
      value: 2500,
      rating: 'needs-improvement',
      delta: 100,
      id: 'test-id',
      url: '/'
    });
  });
});
```

### Integration Tests

```typescript
// packages/web/src/lib/__tests__/integration.test.ts
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { AnalyticsProvider } from '@/providers/AnalyticsProvider';
import { ErrorBoundary } from '@/components/ErrorBoundary';
import { useAnalytics } from '@/hooks/useAnalytics';

describe('Analytics Integration', () => {
  it('provides analytics context to children', () => {
    const TestComponent = () => {
      const { track } = useAnalytics();

      return (
        <button onClick={() => track('test_event')}>
          Track Event
        </button>
      );
    };

    render(
      <AnalyticsProvider>
        <TestComponent />
      </AnalyticsProvider>
    );

    const button = screen.getByText('Track Event');
    fireEvent.click(button);

    // Event should be tracked
    expect(window.umami?.track).toHaveBeenCalledWith('test_event', undefined);
  });

  it('error boundary captures and reports errors', () => {
    const ThrowError = () => {
      throw new Error('Test error');
    };

    const mockCaptureException = jest.fn();
    window.Sentry = { captureException: mockCaptureException };

    render(
      <ErrorBoundary>
        <ThrowError />
      </ErrorBoundary>
    );

    expect(screen.getByText('Something went wrong')).toBeInTheDocument();
    expect(mockCaptureException).toHaveBeenCalledWith(
      expect.any(Error),
      expect.objectContaining({
        contexts: expect.any(Object)
      })
    );
  });
});
```

### Performance Impact Assessment

```typescript
// packages/web/scripts/performance-test.ts
import puppeteer from 'puppeteer';
import lighthouse from 'lighthouse';

async function measurePerformanceImpact() {
  const browser = await puppeteer.launch({ headless: true });

  const results = {
    withAnalytics: {},
    withoutAnalytics: {}
  };

  // Test without analytics
  process.env.NEXT_PUBLIC_ANALYTICS_ENABLED = 'false';
  const page1 = await browser.newPage();
  const report1 = await lighthouse('http://localhost:3000', {
    port: new URL(browser.wsEndpoint()).port,
    output: 'json',
    onlyCategories: ['performance'],
  });
  results.withoutAnalytics = report1.lhr.categories.performance;

  // Test with analytics
  process.env.NEXT_PUBLIC_ANALYTICS_ENABLED = 'true';
  const page2 = await browser.newPage();
  const report2 = await lighthouse('http://localhost:3000', {
    port: new URL(browser.wsEndpoint()).port,
    output: 'json',
    onlyCategories: ['performance'],
  });
  results.withAnalytics = report2.lhr.categories.performance;

  await browser.close();

  // Compare results
  console.log('Performance Impact Analysis:');
  console.log('Without Analytics:', results.withoutAnalytics.score * 100);
  console.log('With Analytics:', results.withAnalytics.score * 100);
  console.log('Impact:', (results.withoutAnalytics.score - results.withAnalytics.score) * 100);

  // Check if impact is acceptable (< 5 point difference)
  const impact = (results.withoutAnalytics.score - results.withAnalytics.score) * 100;
  if (impact > 5) {
    console.error('WARNING: Analytics causing significant performance impact!');
    process.exit(1);
  }

  return results;
}

if (require.main === module) {
  measurePerformanceImpact().catch(console.error);
}
```

### Setup Documentation

```markdown
# Analytics & Error Tracking Setup Guide

## Quick Start

### 1. Prerequisites

- Docker and Docker Compose installed
- Node.js 18+ and pnpm
- At least 2GB available RAM
- Ports 3001 and 3002 available

### 2. Initial Setup

```bash
# Navigate to analytics directory
cd deploy/analytics

# Copy environment template
cp .env.example .env

# Edit .env with your configuration
# IMPORTANT: Generate secure random strings for secrets

# Start services
docker-compose up -d

# Check status
docker-compose ps
```

### 3. Configure Umami

1. Access Umami at http://localhost:3001
2. Login with default credentials (admin/umami)
3. **IMMEDIATELY** change the admin password
4. Add your website:
   - Click "Settings" → "Websites" → "Add website"
   - Name: sbozh.me Development
   - Domain: localhost
5. Copy the website ID from the settings
6. Add to your `.env.local`:
   ```
   NEXT_PUBLIC_UMAMI_WEBSITE_ID=your-website-id
   ```

### 4. Configure GlitchTip

1. Access GlitchTip at http://localhost:3002
2. Create an account (first user becomes admin)
3. Create an organization (e.g., "sbozh")
4. Create a project (e.g., "website")
5. Get your DSN from project settings
6. Add to your `.env.local`:
   ```
   NEXT_PUBLIC_GLITCHTIP_DSN=http://your-key@localhost:3002/1
   ```

### 5. Integrate with Next.js

```bash
# Install dependencies
pnpm add @sentry/nextjs web-vitals

# The provider components are already created
# Just ensure they're imported in your layout.tsx
```

### 6. Test Integration

```bash
# Start development server
pnpm dev

# Visit http://localhost:3000
# Check browser console for "Analytics: Umami loaded"

# Trigger test events
# Visit /test/analytics to see test buttons

# Check dashboards
# Umami: http://localhost:3001
# GlitchTip: http://localhost:3002
```

## Configuration Options

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `NEXT_PUBLIC_ANALYTICS_ENABLED` | Enable/disable analytics | `true` |
| `NEXT_PUBLIC_FORCE_ANALYTICS` | Force analytics in development | `false` |
| `NEXT_PUBLIC_UMAMI_WEBSITE_ID` | Umami website identifier | Required |
| `NEXT_PUBLIC_GLITCHTIP_DSN` | GlitchTip project DSN | Required |

### Privacy Settings

Users can control tracking via the Privacy Settings page at `/privacy`.

### Data Retention

- Analytics data: 90 days
- Error logs: 30 days
- Performance metrics: 7 days
- Session data: Until browser closed

## Troubleshooting

### Services won't start

```bash
# Check logs
docker-compose logs -f

# Common issues:
# - Port conflicts: Change ports in docker-compose.yml
# - Database issues: Remove volumes and restart
docker-compose down -v
docker-compose up -d
```

### No data in dashboards

1. Check browser console for errors
2. Verify environment variables are set
3. Check Content Security Policy allows connections
4. Ensure analytics consent is granted

### High memory usage

```bash
# Check resource usage
docker stats

# Limit memory in docker-compose.yml
services:
  umami:
    mem_limit: 512m
```

## Security Considerations

1. **Change default passwords immediately**
2. Use strong, unique secrets for:
   - `UMAMI_APP_SECRET`
   - `GLITCHTIP_SECRET_KEY`
   - Database passwords
3. Restrict database access to local network
4. Regular backups of analytics data
5. Monitor for suspicious activity

## Maintenance

### Daily Tasks
- Monitor error rates in GlitchTip
- Check service health status

### Weekly Tasks
- Review analytics reports
- Clear resolved errors
- Check disk usage

### Monthly Tasks
- Update Docker images
- Review and adjust alert thresholds
- Backup configuration and data

### Backup

```bash
# Backup databases
docker exec analytics-postgres pg_dumpall -U analytics_admin > backup.sql

# Restore from backup
docker exec -i analytics-postgres psql -U analytics_admin < backup.sql
```
```

### Migration Guide for 0.10.0

```markdown
# Migration Guide: 0.9.x to 0.10.0 (Production Deployment)

## Overview

Version 0.10.0 focuses on production deployment to Hetzner VPS with proper security, monitoring, and scalability.

## Pre-Migration Checklist

- [ ] All 0.9.x features tested and working locally
- [ ] Analytics collecting data correctly
- [ ] Error tracking capturing issues
- [ ] Privacy controls functional
- [ ] Performance impact acceptable (< 5% degradation)
- [ ] Documentation updated

## Migration Steps

### 1. Prepare Production Environment

```bash
# On Hetzner VPS
# Update system
apt update && apt upgrade -y

# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# Install Docker Compose
apt install docker-compose -y

# Create deployment user
adduser deploy
usermod -aG docker deploy
```

### 2. Update Configuration for Production

Create `deploy/production/docker-compose.yml`:

```yaml
version: '3.8'

services:
  # Include all services from analytics
  # Add production-specific configuration:
  # - Traefik for reverse proxy
  # - SSL certificates
  # - Resource limits
  # - Health checks
  # - Restart policies
```

### 3. Environment Variables for Production

```bash
# deploy/production/.env
NODE_ENV=production
NEXT_PUBLIC_APP_URL=https://sbozh.me

# Analytics
NEXT_PUBLIC_UMAMI_WEBSITE_ID=prod-website-id
NEXT_PUBLIC_UMAMI_SCRIPT_URL=https://analytics.sbozh.me/script.js

# Error Tracking
NEXT_PUBLIC_GLITCHTIP_DSN=https://key@errors.sbozh.me/2
```

### 4. SSL Configuration

```bash
# Using Traefik with Let's Encrypt
# Add to docker-compose.yml
labels:
  - "traefik.http.routers.umami.tls.certresolver=letsencrypt"
  - "traefik.http.routers.glitchtip.tls.certresolver=letsencrypt"
```

### 5. Database Migration

```bash
# Export local data
docker exec analytics-postgres pg_dump -U admin umami_db > umami_backup.sql
docker exec analytics-postgres pg_dump -U admin glitchtip_db > glitchtip_backup.sql

# Import to production
scp *.sql deploy@server:/home/deploy/
ssh deploy@server
docker exec -i prod-postgres psql -U admin < umami_backup.sql
docker exec -i prod-postgres psql -U admin < glitchtip_backup.sql
```

### 6. Update DNS Records

```
analytics.sbozh.me    A    your-server-ip
errors.sbozh.me       A    your-server-ip
```

### 7. Deploy and Verify

```bash
# Deploy
cd deploy/production
docker-compose up -d

# Verify
curl -I https://analytics.sbozh.me
curl -I https://errors.sbozh.me

# Check logs
docker-compose logs -f
```

## Rollback Plan

If issues arise:

```bash
# Stop production services
docker-compose down

# Restore previous version
git checkout v0.9.5
docker-compose up -d

# Restore database if needed
docker exec -i prod-postgres psql -U admin < backup_previous.sql
```

## Post-Migration Tasks

- [ ] Update production monitoring alerts
- [ ] Configure backup automation
- [ ] Set up log aggregation
- [ ] Update privacy policy with production URLs
- [ ] Performance testing on production
- [ ] Security audit

## Breaking Changes

None expected. The migration is primarily infrastructure-focused.

## Support

For issues during migration:
1. Check logs: `docker-compose logs service-name`
2. Review documentation in `/deploy/production/README.md`
3. Contact team via Slack #deployment channel
```

## Files to Create

| File | Purpose |
|------|---------|
| `packages/web/e2e/analytics.spec.ts` | E2E tests for analytics |
| `packages/web/src/lib/analytics/__tests__/*.test.ts` | Unit tests |
| `packages/web/scripts/performance-test.ts` | Performance impact testing |
| `deploy/analytics/SETUP.md` | Complete setup guide |
| `deploy/analytics/MIGRATION.md` | 0.10.0 migration guide |
| `deploy/analytics/TROUBLESHOOTING.md` | Common issues and solutions |

## Files to Modify

| File | Changes |
|------|---------|
| `package.json` | Add test scripts and dependencies |
| `playwright.config.ts` | Configure E2E test settings |
| `README.md` | Add analytics section |
| `PRIVACY.md` | Update with tracking details |

## Acceptance Criteria

- [ ] All E2E tests passing
- [ ] Unit test coverage > 80%
- [ ] Performance impact < 5%
- [ ] Setup documentation complete
- [ ] Migration guide reviewed
- [ ] Privacy policy updated
- [ ] Monitoring playbook created
- [ ] Team training completed
- [ ] Backup strategy documented
- [ ] Security audit passed

## Testing

```bash
# Run all tests
pnpm test:analytics

# E2E tests
pnpm test:e2e:analytics

# Unit tests with coverage
pnpm test:unit:coverage

# Performance impact test
pnpm test:performance

# Integration tests
pnpm test:integration

# Full test suite
pnpm test:all
```

## Notes

- Document all tracking events for GDPR compliance
- Regular testing ensures tracking accuracy
- Performance monitoring prevents degradation
- Clear documentation reduces onboarding time
- Migration guide critical for smooth 0.10.0 transition